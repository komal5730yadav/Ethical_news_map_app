{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "efab09ec",
      "metadata": {
        "id": "efab09ec"
      },
      "outputs": [],
      "source": [
        "!python -m spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5faee9dc",
      "metadata": {
        "id": "5faee9dc"
      },
      "outputs": [],
      "source": [
        "!pip install folium pandas ipyleaflet geopy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63a9eaa4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63a9eaa4",
        "outputId": "7efa3c07-8c39-4e99-efac-3bff2e34fcad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your search terms separated by commas: ai, ethical, issues\n",
            "News fetched for keywords: ai,  ethical,  issues\n",
            "Total execution time: 28.209688186645508\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "import xml.etree.ElementTree as ET\n",
        "import time\n",
        "import spacy\n",
        "import geopy\n",
        "from bs4 import BeautifulSoup\n",
        "from geopy.geocoders import Nominatim\n",
        "import datetime as dt\n",
        "\n",
        "def clean_url(searched_items, data_filter):\n",
        "    \"\"\"\n",
        "    OUTPUT : url to be fetched for the searched_item and data_filter\n",
        "    ---------------------------------------------------\n",
        "    Parameters:\n",
        "      today' - get headlines of the news that are released only in today\n",
        "      'this_week' - get headlines of the news that are released in this week\n",
        "      'this_month' - news released in this month\n",
        "      'this_year' - news released in this year\n",
        "      number : int/str input for number of days ago\n",
        "      or '' blank to get all data\n",
        "    \"\"\"\n",
        "    x = dt.datetime.today()\n",
        "    today = str(x)[:10]\n",
        "    yesterday = str(x + pd.Timedelta(days=-1))[:10]\n",
        "    this_week = str(x + pd.Timedelta(days=-7))[:10]\n",
        "    if data_filter == 'today':\n",
        "        time = 'after%3A' + yesterday\n",
        "    elif data_filter == 'this_week':\n",
        "        time = 'after%3A' + this_week + '+before%3A' + today\n",
        "    elif data_filter == 'this_year':\n",
        "        time = 'after%3A' + str(x.year - 1)\n",
        "    elif str(data_filter).isdigit():\n",
        "        temp_time = str(x + pd.Timedelta(days=-int(data_filter)))[:10]\n",
        "        time = 'after%3A' + temp_time + '+before%3A' + today\n",
        "    else:\n",
        "        time = ''\n",
        "\n",
        "    # Construct the query with multiple keywords\n",
        "    query = '+'.join(searched_items)\n",
        "    url = f'https://news.google.com/rss/search?q={query}+' + time + '&hl=en-US&gl=US&ceid=US%3Aen'\n",
        "    return url\n",
        "\n",
        "def extract_locations(text):\n",
        "    \"\"\"\n",
        "    Extracts location entities from the given text using spaCy's NER.\n",
        "    \"\"\"\n",
        "    # Load English tokenizer, tagger, parser, NER, and word vectors\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "    # Process the text with spaCy\n",
        "    doc = nlp(text)\n",
        "\n",
        "    # Extract location entities\n",
        "    locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE' and ent.text.lower() not in ['openai', 'genai']]\n",
        "\n",
        "    return locations\n",
        "\n",
        "def geocode_location(location):\n",
        "    \"\"\"\n",
        "    Geocodes the given location to get latitude and longitude coordinates.\n",
        "    \"\"\"\n",
        "    geolocator = Nominatim(user_agent=\"news_app\")  # You can change the user_agent\n",
        "    try:\n",
        "        location_info = geolocator.geocode(location)\n",
        "        latitude = location_info.latitude\n",
        "        longitude = location_info.longitude\n",
        "    except:\n",
        "        latitude = \"NA\"\n",
        "        longitude = \"NA\"\n",
        "    return latitude, longitude\n",
        "\n",
        "def get_news_for_keywords(keywords, data_filter=None):\n",
        "    \"\"\"\n",
        "    Search through Google News with the \"keywords\" and get the headlines\n",
        "    and the contents of the news that was released today, this week, this month,\n",
        "    or this year (\"date_filter\").\n",
        "    \"\"\"\n",
        "\n",
        "    url = clean_url(keywords, data_filter)\n",
        "    response = requests.get(url)\n",
        "    # get the root directly as we have text file of string now\n",
        "    root = ET.fromstring(response.text)\n",
        "\n",
        "    # Filter news articles containing all keywords\n",
        "    all_keywords_found = []\n",
        "    for item in root.findall('.//channel/item'):\n",
        "        title = item.find('title').text.lower()\n",
        "        if all(keyword.lower() in title for keyword in keywords):\n",
        "            all_keywords_found.append(item)\n",
        "\n",
        "    # get the required data\n",
        "    title = [i.find('title').text for i in all_keywords_found]\n",
        "    link = [i.find('link').text for i in all_keywords_found]\n",
        "    pubDate = [i.find('pubDate').text for i in all_keywords_found]\n",
        "    source = [i.find('.//source').text for i in all_keywords_found]\n",
        "\n",
        "    # Extract locations from the descriptions\n",
        "    locations = []\n",
        "    latitudes = []\n",
        "    longitudes = []\n",
        "    for i in all_keywords_found:\n",
        "        # Get the full content of the news article from the source link\n",
        "        article_response = requests.get(i.find('link').text)\n",
        "        article_html = article_response.text\n",
        "\n",
        "        # Extract text content from HTML using BeautifulSoup\n",
        "        soup = BeautifulSoup(article_html, 'html.parser')\n",
        "        article_text = ' '.join([p.text for p in soup.find_all('p')])\n",
        "\n",
        "        # Extract locations from the entire article content\n",
        "        article_locations = extract_locations(article_text)\n",
        "        locations.append(','.join(article_locations) if article_locations else 'NA')\n",
        "\n",
        "        if article_locations:\n",
        "        # Geocode only the first location to get latitude and longitude\n",
        "            latitude, longitude = geocode_location(article_locations[0])\n",
        "        else:\n",
        "            latitude, longitude = \"NA\", \"NA\"\n",
        "\n",
        "        # Geocode locations to get latitude and longitude\n",
        "        latitudes.append(latitude)\n",
        "        longitudes.append(longitude)\n",
        "\n",
        "    # set the data frame\n",
        "    df = pd.DataFrame({'title': title, 'link': link, 'date': pubDate, 'source': source, 'locations': locations, 'latitude': latitudes, 'longitude': longitudes})\n",
        "    df['keywords'] = ','.join(keywords)  # Add a column to identify the keywords\n",
        "    # adjust the date column\n",
        "    df.date = pd.to_datetime(df.date, unit='ns')\n",
        "    return df\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    start = time.time()\n",
        "    keywords = input('Enter your search terms separated by commas: ').split(',')\n",
        "    data_filter = \"this_month\"\n",
        "\n",
        "    data = get_news_for_keywords(keywords, data_filter)\n",
        "    print(f\"News fetched for keywords: {', '.join(keywords)}\")\n",
        "\n",
        "    data.to_csv('news_data.csv', encoding='utf-8-sig', index=False)\n",
        "    end = time.time() - start\n",
        "    print(\"Total execution time:\", end)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import folium\n",
        "import pandas as pd\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# Load your dataset\n",
        "data = pd.read_csv('news_data.csv')\n",
        "\n",
        "# Create a map centered around an average location of your data\n",
        "map_center = [data['latitude'].mean(), data['longitude'].mean()]\n",
        "mymap = folium.Map(location=map_center, zoom_start=10)\n",
        "\n",
        "# Create a dictionary to store articles by location\n",
        "articles_by_location = {}\n",
        "\n",
        "# Iterate through the dataset\n",
        "for index, row in data.iterrows():\n",
        "    # Check if latitude and longitude are valid\n",
        "    if pd.notnull(row['latitude']) and pd.notnull(row['longitude']):\n",
        "        # Extract the first location from the list\n",
        "        location = row['locations'].split(',')[0].strip()\n",
        "        # Check if location is not NA\n",
        "        if location != 'NA':\n",
        "            # Add article to the dictionary for the current location\n",
        "            if location not in articles_by_location:\n",
        "                articles_by_location[location] = []\n",
        "            articles_by_location[location].append({'title': row['title'], 'date': row['date'], 'link': row['link']})\n",
        "\n",
        "# Add circles to the map with customized popups\n",
        "for location, articles in articles_by_location.items():\n",
        "    # Geocode location to get latitude and longitude\n",
        "    try:\n",
        "        latitude, longitude = geocode_location(location)\n",
        "        popup_content = f'<div><strong>Location:</strong> {location}<br>'\n",
        "        # Add articles to the popup content\n",
        "        for article in articles:\n",
        "            popup_content += f'''\n",
        "            <p><strong>Title:</strong> {article[\"title\"]}</p>\n",
        "            <p><strong>Date:</strong> {article[\"date\"]}</p>\n",
        "            <p><a href=\"{article[\"link\"]}\" target=\"_blank\">Go to Source</a></p>\n",
        "            '''\n",
        "        popup_content += '</div>'\n",
        "        popup = folium.Popup(popup_content, max_width=300)  # Max width for better display\n",
        "        folium.CircleMarker(location=[latitude, longitude], radius=10, popup=popup, color='blue', fill=True).add_to(mymap)\n",
        "    except ValueError as e:\n",
        "        print(f\"Skipping location '{location}': {e}\")\n",
        "\n",
        "# Save the map to an HTML file\n",
        "html_file = 'map_with_custom_tooltips_clickable.html'\n",
        "mymap.save(html_file)\n",
        "\n",
        "# Read the HTML file and parse it using BeautifulSoup\n",
        "with open(html_file, 'r') as f:\n",
        "    html_content = f.read()\n",
        "    soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Insert the header content with a white opaque box\n",
        "header_content = '''\n",
        "<div style=\"position: absolute; top: 15px; left: 50%; transform: translateX(-50%); z-index: 1000;\">\n",
        "    <div style=\"background-color: rgba(255, 255, 255, 0.6); padding: 8px; border-radius: 14px;\">\n",
        "        <h1 style=\"color: black; font-weight: bold; font-family: Helvetica, sans-serif;font-size: 28px;\">Global AI Ethical Issues</h1>\n",
        "    </div>\n",
        "</div>\n",
        "'''\n",
        "header_tag = soup.new_tag('div')\n",
        "header_tag.append(BeautifulSoup(header_content, 'html.parser'))\n",
        "soup.body.insert(0, header_tag)\n",
        "\n",
        "# Write the updated HTML content back to the file\n",
        "with open(html_file, 'w') as f:\n",
        "    f.write(str(soup))\n"
      ],
      "metadata": {
        "id": "MhHw2MCIaEof"
      },
      "id": "MhHw2MCIaEof",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}